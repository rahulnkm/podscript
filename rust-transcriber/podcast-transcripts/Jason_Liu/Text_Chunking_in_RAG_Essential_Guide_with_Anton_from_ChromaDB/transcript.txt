Listen, if you have built any AI applications or any kind of RAG applications, you know that chunking is a big deal, but most people don't know what they're doing. And so today I'm really excited to invite my friend, Anton from ChromaDB to talk a little bit more about their chunking research from their technical reports. Chunking strategy matters a lot. It significantly influences the performance of your retrieval system. We're also gonna talk about the most important rule for any AI engineer, thinking about evals. Always, always, always look at your data and more importantly, what to look for. Recall is probably the single most important metric. The models are improving at ignoring irrelevant information, but they cannot do the task if you haven't given them all the relevant information. Let's talk about chunking. I think chunking is on a lot of people's minds, but I actually don't think there's been a lot of work put into this in figuring out what actually works and what doesn't in practical deployments of retrieval. So Jason, as you mentioned, we've been putting out some technical reports and our technical reports are oriented to basically help practitioners actually understand how to improve their systems. And this is the sort of approach we took with our chunking work as well. I think that that's important to recognize. That's a starting point here. And I'll share the URL or the full technical report at the end, but I just wanna walk you through the main pieces of this and sort of the conclusions that we've drawn and the things that we need to pay attention to. We might as well get started. So I think it's important to start thinking about from the point of like, what is chunking? I think this is something that we actually take for granted a lot because you have a giant document, you feed it into a chunker, it gives you chunks and then you retrieve among the chunks, but what are we actually doing here? What is it for? There's a few reasons why we use chunking, but what chunking actually is, is you take a document and you split it up into its components. And then what you're looking for in any given sort of, I mean, in the retrieved results for a given query is to find the relevant text if we're working with text data to that query among all the chunks of that document, among all the divisions that we've created of that document. So that's the basic idea of chunking. And I think there's a few important things here. It's important to note, for example, that the relevant results to a particular query may not appear in the same document. So for example, if you have a summarization task across a large corpus, you're actually looking for information that appears across documents. And here it means that your data needs to appear among a variety of chunks, not just adjacent ones in one particular document. This diagram is hopefully fairly illustrative. I think we also need to think about why chunk. Every so often this idea comes out that, oh, with long context windows, retrieval is completely dead, it's pointless to do it. So, and I think a lot of people have in the back of their head, they're like, oh, well, the reason that we would do chunking is because the context window of the LLM is a finite length, but there's actually a bunch of good reasons to do that regardless of the length of the model's context window. The first is, even if the LLM's context window size is growing, the embedding model input size tends to remain fairly constant, right? And this is something that trips a lot of people up. Typically, embedding models that you say download from HuggingFace will not tell you if you're exceeding their input context window. It'll just truncate your document. And then people will be like, well, why are my retrieval results so bad? Is something wrong with my embedding model? The reality is you're probably feeding it documents that are too large. Or chunks, I should say, that are too large. The other piece is inference efficiency. So despite the fact that you might have these large language models which have a wide context window, you're still paying in terms of inference time and in terms of cost per token. And so what that means is, in the ideal case, you only want the model to process exactly and only the relevant information to the query or the instruction that you're asking it to process or perform, right? And so chunking aids us in retrieving only the relevant information instead of all the available information. Now, the other piece to chunking that really improves the generative capability of these systems is information accuracy and the elimination of distractors. So needle-in-a-haystack evals are really good at illustrating like, oh, can the model find this one piece of information among all the information that it has in its context window? And it's one way to evaluate the effectiveness of the model in its context. But it's very easy to confuse the model and to have it perform the task not in a way that you expect or to process the wrong information because it needs to combine not just finding information in its context window but with the instruction that you actually want it to perform. So effective chunking helps you eliminate those distractors that produce poor model outputs. Then finally, effective chunking actually goes a long way to improving retrieval performance. So what I mean by that is effective chunking actually means that your retrieval system is more likely to find all the relevant information that you need for a particular query. All these things are actually really, really important and will continue to be important no matter how long the context windows gap. So let's talk about the typical approaches to chunking. There's kind of a couple of broad categories here. Typically, the things that are most widely used today are the heuristic approaches, right? So what I mean by that is basically you have some heuristic about how to divide documents into smaller pieces. Typically, you would use something like separator characters. So for example, double new lines or question marks, exclamation marks, full stops, things like that are sort of natural encodings of where pieces of text are divided. They basically rely on the existing structure of the document. They generally produce good results as long as your documents are fairly clean in the first place. As soon as you start throwing special characters or different symbols in there, the heuristic methods can really have a problem. And that goes to show that heuristics are typically fairly brittle and they do require a lot of sort of pre-processing and cleaning of those documents ahead of time. Probably the most widely used chunking strategy right now is the recursive character text splitter. So the idea is you'd give it an upper bound on how long you want your chunk to be. And then it uses, in order, basically a hierarchy of splitting characters to subdivide each document into pieces not longer than the specified maximal length, right? And this is implemented in Lang chain and several other places. It's actually very straightforward. You can implement this yourself without much work. I do recommend giving it a try. It's good to get intuition about what's really going on under the hood by really giving these a shot. And I'll return to that point a little bit later. One thing that's interesting that started to emerge and we've been doing experiments with this ourselves, it's still very early, however, is using the embedding our language models directly to find semantic boundaries of documents. And so what I mean by semantic boundary is finding those points in a document where it's no longer talking about the thing it was talking about in the previous section. This is a way to avoid the brittleness of the heuristics by actually looking at the meaning and the content of each chunk as opposed to what characters separate them. The advantage here is actually really interesting because this works, this can work, I should say, using the same embedding model that you're using to embed the chunk into your retrieval system as well. So in some sense, you can find an almost embedding model optimal chunking strategy if you apply these heuristic chunking approaches. This is still fairly experimental, but we're seeing good early results. And because embeddings are relatively cheap, actually getting these semantic boundaries out using an embedding model tends to work pretty well. And you can imagine some hybrid system where you use the heuristic as sort of a weight function over the semantic chunker as well. That's something we haven't done a lot of experimentation with, but it's an obvious next step. Great. Does chunking strategy matter? This is the main outcome of our recent technical report on chunking. Chunking strategy matters a lot. I'm not gonna go through this table in detail, but the takeaway here is it matters a lot. It significantly influences the performance of your retrieval system. And we found a lot of sort of interesting results along the way of doing this work. Hey, are your RAG applications falling short despite using the latest frameworks and models? You're not alone. I'm Jason Liu and I've helped a dozen companies improve their AI applications. I've developed a systematic approach that delivers results. If you wanna check it out, join my free six week email course and learn the techniques I use in my own consulting practice. That way, you can stop guessing at what works and measure what matters. Just check out improvingrag.com. So there's a couple of rules of thumb that we found out. The first thing is that you really want to fill the embedding model context window as much as you can. So try to fill it up to its maximum character limit. We found that often strategies that produce very short chunks essentially are very, very noisy in terms of their retrieval accuracy. Typically, if you have a short average chunk length across your corpus, it's unlikely that you're going to be getting good results regardless of which embedding model that you use. You should be trying to fill that embedding model's context window as much as you can. But the trade-off is you really don't wanna group unrelated information together. You can think of the embedding model as something that is attempting to essentially summarize the meaning of a particular text and then encode it into a point in latent space. It's very difficult to summarize the meaning of a chunk which contains contradictory or differing information. But even worse than that, you can have really poor results if you've grouped information that's very similar but not identical. So for example, if you're looking at a company report and you're looking at information from the year 2022 and the year 2023, this thing that the report is talking about is gonna be very similar. But for example, the revenue numbers might be very different. And so grouping those together into a single chunk might produce really poor results. That's just an example. So these are the two rules of thumb and these two things are in tension with one another. But the real thing that I really wanna get across here is always, always, always look at your data. Look at your data. So for example, the way that we did this work with this table here is we produced a generative benchmark and I'll go into some detail about that in a minute where we basically feed it arbitrary corpus of text, have it generate queries that refer to specific passages in the text as opposed to looking at just the retrieved documents and then applying different chunking strategies under the same embedding model to see how recall essentially performs. And what we found is by default, a lot of the heuristic chunkers produce chunks that are way too short. And we only discovered this by actually looking at the data. We were trying to figure out why are we getting these 50 token chunks out of some of the most widely used chunking strategies? And it's because the delimiter characters used in the hierarchical chunker are just not, they're in the wrong order by default and they're also the wrong characters by default. They produce very poor chunking output. So always look at your data, regardless of what chunking strategy you go to pursue, take a look, right? You can actually develop intuition and the whole point of semantic retrieval is it's looking for information the same way that a human would. So you taking a look at the data and developing intuition about how your chunking strategy is actually working for your use case will help you a lot. We're developing tooling that should help with this. We've got a lot of kind of evaluation tooling in the pipeline to help you continuously improve the retrieval accuracy of your systems. In particular, we're focusing on the recall and this is likely to be a part of that suite where we can use some semantic modeling to actually help you find outliers. But again, coming back to this, just look at your data, look at your data. You will, if you're getting poor performance and your chunks look weird, that's probably what's happening. There's something happening in the way that you're going about chunking and that's producing something as surprising result. So the question is then like, how do you actually choose among chunking strategies? And I think because people don't have an effective way of doing this and feel like that it would be a lot of effort, they mostly go with the defaults. But as we've seen, the defaults can be pretty poor and I think it's not unreasonable to expect that the chunking strategy that you use actually depends pretty strongly on the tasks that you actually want to perform. This means that, for example, there's probably not one universal chunking strategy that is the best. There is a chunking strategy that is good for your data and your task. When evaluating chunking, I would, we went about evaluating this at the level of the retriever itself rather than the generative output in this use case. And this is also fairly different to what you see from traditional information retrieval benchmarks. Recall is probably the single most important metric. And the reason for that is because the models are improving at ignoring irrelevant information, but they cannot do the task if you haven't given them all the relevant information. So the recall is probably the most important piece here. And the way that you measure recall is also quite important. You should be measuring the recall of relevant passages rather than was any passage from a document that we've labeled relevant retrieved. In traditional information retrieval benchmarks, they're typically document level because traditional information retrieval is about whole document retrieval. And it doesn't take into account, for example, that we only want a certain subset of the text that's getting retrieved. And that entire document might be getting retrieved not because a relevant piece of it's getting retrieved, but because the embedding model found something else and it just happened to do that. It's a false positive. You need to be evaluating on the level of relevant passages, as well as because like a lot of your queries are going to be cross document queries, right? The results are gonna lie across multiple documents. And the other thing that I wanna say about traditional information retrieval benchmarks is they're often taken into account. So metrics like NDCG take into account the ranking of the retrieved documents, which means like the order of retrieved documents influences the final score of the retrieval system, less relevant in retrieval augmented generation. Because again, the model is pretty good at extracting all the relevant information that it needs as long as that information is available to it. So look at the raw recall score and look at it at a passage level. We have code for this out. The same benchmark that we used in our technical report is available. It's a generative benchmark, so it'll work against your data. I encourage you to give it a try. If it's doing something you don't like, feel free to open a pull request or open an issue. We're actively developing it. The other thing that I think is really important here is retrieval is not, especially in the way that we use it in AI application development, is not a general system. It is, again, and I really wanna stress this, it is dependent on your task. It is dependent on your data, which means the evaluation that you perform needs to depend on the sorts of queries that you expect to see. There's no universal evaluation of recall. Things like the Massive Text Embedding Benchmark are great if your application is Google Search or you're doing a web scale recommender system, but that's just not what we're building here. We are thinking of these things as the backing database for an application. And so our retrieval accuracy is going to depend on what our application actually is. And I think this is sort of emerging as a practice. There's not great tooling for it yet. We're hoping to contribute here, but you can rely pretty well on generative tools to perform evaluation at scale. We've provided code for the way we've done our generative benchmark. The caveat here is, again, look at your data. You have to look at your data. At every step of everything that you're doing in evaluating your trunking strategy or your retrieval system in general is always, always look at your data. I'm gonna put this slide up again because I really wanna hammer this in. This is, we're in an interesting era right now of software development, where people who want to build software applications with AI are kind of being forced to learn a lot of things which have been rules of thumb in the machine learning community for a while for the first time. And so we really need to hammer on these. I actually think the real solution here is just better tooling, because I don't think it's reasonable for every software engineer to suddenly know all the tricks of the trade that we've developed over the last 15 to 20 years in ML. But in the meantime, looking at your data is going to help you a lot. So always, always look at your data, always look at your data. If I find, if you come to me with a problem and you haven't looked at your data, I will laugh at you. Thanks for watching. If you liked that video, make sure to like and subscribe. And if you wanna learn more, check out improvingrag.com. We're gonna talk about the foundations and not frameworks. Over six emails, you're gonna learn how to overcome absence blindness, as well as how to fine tune language models, understanding diverse query types, improving multimodal RAG, and ultimately smart question routing and build a UX that collects user feedback. Just check out improvingrag.com and fill out this quick email address at the bottom. Thanks.
